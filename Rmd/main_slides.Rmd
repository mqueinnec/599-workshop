---
title: "Overview of modelling in R"
subtitle: "FCOR599 Workshop"
author: "Sarah Smith-Tripp, Martin Queinnec"
date: "18/02/2022"
output:
  slidy_presentation: default
  ioslides_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages 

```{r attach packages, echo=TRUE, message=FALSE}
library(palmerpenguins)
library(lme4)
library(MuMIn)
library(randomForest)
library(caret)
library(cowplot)
library(tidyverse)
```

## Goal of today 

- High-level overview of modelling

- Linear models and random forest models in R

- Assessing a model accuracy 

## Modelling

- Your goal is to predict a **response** variable from a set of **predictor** variables

- You collected observations of response and predictor variables

- You want to use this data to model the response variable from the predictor variables

- Eventually, you might want to use this model to predict the response variable for new data

## Some considerations

- Is the response variable **continuous** or **discrete**? 

- **Regression** models are used for continuous response variables

- **Classification** models are used for discrete response variables

- Are your predictor variables continuous, discrete or a mix of both? 

- All these considerations are important when choosing a type of model

## Modeling in R

- Many functions/packages exist for different types of models

- The base `stats` packages implements basic models (e.g. `lm`)

- Most models are a syntax similar to: 

```{r, eval = FALSE}
my_model <- model_fun(formula = y ~ 1 + x1 + x2, 
                      data = my_data, 
                      ...)

my_model <- model_fun(x = pred_df, 
                      y = resp_obs, 
                      ...)
```

- For most models, predictions can be made with the function `predict()`

```{r, eval = FALSE}
predict(my_model, newdata)
```

## Palmer Penguins Dataset

![](https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png){width=80%}
![](https://allisonhorst.github.io/palmerpenguins/reference/figures/culmen_depth.png){width=80%}

```{r, echo=FALSE}
dat <- penguins %>% 
  filter(across(.cols = everything(), ~!is.na(.x)))
```

```{r}
head(dat)
```

## Linear model

- Let's assume that there is a linear relationship between body mass and bill / flipper dimensions

$Y_{i} = a + \sum_{i}^{u} b_{i} \times X_{i}$


```{r}
my_lm_model <- lm(body_mass_g ~ 1 + bill_length_mm + bill_depth_mm + flipper_length_mm, 
   data = dat)

summary(my_lm_model)
par(mfrow = c(1,4))
plot(my_lm_model)
par(mfrom = c(1,1))
```

## Linear model with random effects
- Random effects can fit either a random intercept or random slope
- Adjust for issues of spatial temporal corellation
- use a slight different structure 

## Why perform random effects?
- we assume that there will be variability in the relationship as a component of random effect
- look at the data 

```{r}
ggplot(dat, aes(body_mass_g, group = species )) + geom_density(aes(fill= species),alpha = 0.4) + theme_bw()

```

## Building random effect models 
```{r}
my_random_intercept_model <- lmer(body_mass_g ~   bill_length_mm + (1|species), data = dat)
my_random_slope_model <- lmer(body_mass_g ~   bill_length_mm + (bill_length_mm|species), data = dat)


ggplot(dat, aes(group = species, color = species)) + geom_point(aes(x = bill_length_mm, y = body_mass_g)) + 
  geom_line(aes(bill_length_mm, predict_random)) + theme_bw()
```

##checking models of random effects 
-check model fits
```{r}
summary(my_random_slope_model)
summary(my_random_intercept_model)
```

```{r, echo = F}
Model_Checking_data <- dat %>% 
  mutate(fit = fitted(my_random_intercept_model),
         resid_mod= resid(my_random_intercept_model),
         theorectical_quantiles = ppoints(resid_mod),
         sample_quantiles = sort(rnorm(length(resid_mod), mean(resid_mod), sd(resid_mod))))

qqplot_test <- ggplot(Model_Checking_data) + geom_point(aes(
  sample_quantiles, theorectical_quantiles
)) + theme_bw()

resid_graph <-  ggplot(Model_Checking_data) + 
  geom_point(aes(fit, resid_mod, color = species)) + geom_abline(aes(intercept = 0, slope = 0)) + theme_bw()
plot_grid(qqplot_test, resid_graph, rel_widths = c(1, 1.5))
```
- compare models with log likelihood test
```{r}
anova(my_random_slope_model, my_random_intercept_model)

MuMIn::r.squaredGLMM(my_random_intercept_model)
### R2m is fixed effects only , R2c is mixed + random 

```


-Nakagawa, S., Johnson, P.C.D., Schielzeth, H., 2017. The coefficient of determination R2 and intra-class correlation coefficient from generalized linear mixed-effects models revisited and expanded. Journal of The Royal Society Interface 14, 20170213. https://doi.org/10.1098/rsif.2017.0213




## Random Forest 

- Random Forest is a type of model that can be used for both regression and classification 

- Forest of classification or regression trees

- At each node, **mtry** predictor variables are randomly selected and the tree splits the data minimizing the error 

![](https://www.tibco.com/sites/tibco/files/media_entity/2021-05/random-forest-diagram.svg){width=80%}

## Random Forest - Regression

```{r}
pred_vars_df <- dat %>%
  select(species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, sex)

rf_reg <- randomForest(x = pred_vars_df, 
                       y = dat$body_mass_g)

rf_reg
```

## Random Forest - Classification

- You need to make sure that the response variable is a **factor** (categorical data)

```{r}
pred_vars_df <- dat %>%
  select(island, bill_length_mm, bill_depth_mm, flipper_length_mm, sex, body_mass_g)

rf_class <- randomForest(x = pred_vars_df, 
                       y = dat$species)

rf_class
```

## Random Forest - Variable importance

- It is possible to derive a measure of varaible importance from a random forest model

- If all the observations for a given predictor variable are permuted, what is the impact on the accuracy of the model? 

```{r}
importance(rf_class)

varImpPlot(rf_class)
```

## Accuracy metrics

We assess the accuracy of a model by comparing the observed values ($y_{i}$) to the predicted values ($\hat{y}_{i}$)

Regression: 

- Coefficient of determination: $R^{2}$

- Root mean square error: $RMSE = \sqrt{\frac{\sum{(y_{i} - \hat{y}_{i})^{2}}}{n}}$

- Mean error (bias): $ME = \frac{\sum{(y_{i} - \hat{y}_{i}})}{n}$

Classification: 

- Overall, Producer, User accuracy 

- Kappa coefficient

## Training and validation sets

- It is important to assess the accuracy of a model of unseen data 

- Most obvious way is to split the observations into training and validation (testing) sets

![](https://community.alteryx.com/t5/image/serverpage/image-id/71542i222AF143484A2306/image-size/large?v=v2&px=999){width=80%}

- We can also use *cross-validation* (e.g. k-fold CV with k = 4)

![](https://community.alteryx.com/t5/image/serverpage/image-id/71553i43D85DE352069CB9/image-size/large?v=v2&px=999){width=80%}

## Scatterplot of predicted VS observed values

```{r, echo = FALSE}
my_lm_model <- lm(body_mass_g ~ 1 + bill_length_mm + bill_depth_mm + flipper_length_mm + sex, 
   data = dat)

lm_preds <- my_lm_model$fitted.values
lm_obs <- my_lm_model$model$body_mass_g

lm_df <- data.frame(obs = my_lm_model$model$body_mass_g, 
                    pred = my_lm_model$fitted.values)
```

```{r}
ggplot(lm_df, aes(x = pred, y = obs)) + 
  geom_point()+ 
  geom_abline(linetype = "dashed") + 
  coord_equal() + 
  theme_bw() + 
  labs(x = "Predicted body mass (g)", 
       y = "Obsverved body mass (g)")
```

---
title: "Overview of modelling in R"
subtitle: "FCOR599 Workshop"
author: "Sarah Smith-Tripp, Martin Queinnec"
date: "18/02/2022"
output:
  ioslides_presentation: default
  slidy_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages 

```{r attach packages, echo=TRUE, message=FALSE}
library(palmerpenguins)
library(lme4)
library(MuMIn)
library(randomForest)
library(caret)
library(cowplot)
library(tidyverse)
```

## Goal of today 

- Your goal is to predict a **response** variable from a set of **predictor** variables

- You collected observations of response and predictor variables

- You want to use this data to model the response variable from the predictor variables

- We will take the example of linear models, linear mixed-effects models and random forest models

- Eventually, you might want to use this model to predict the response variable for new data

## Some considerations

- Is the response variable **continuous** or **discrete**? 

- **Regression** models are used for continuous response variables

- **Classification** models are used for discrete response variables

- Are your predictor variables continuous, discrete or a mix of both? 

- All these considerations are important when choosing a type of model

## Modeling in R

- Many functions/packages exist for different types of models

- The base `stats` packages implements basic models (e.g. `lm`)

- Most models are a syntax similar to: 

```{r, eval = FALSE}
my_model <- model_fun(formula = y ~ 1 + x1 + x2, 
                      data = my_data, 
                      ...)

my_model <- model_fun(x = pred_df, 
                      y = resp_obs, 
                      ...)
```

- For most models, predictions can be made with the function `predict()`

```{r, eval = FALSE}
predict(my_model, newdata)
```

## Palmer Penguins Dataset

![](https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png){width=80%}
![](https://allisonhorst.github.io/palmerpenguins/reference/figures/culmen_depth.png){width=80%}

```{r, echo=FALSE}
dat <- penguins %>% 
  filter(across(.cols = everything(), ~!is.na(.x)))
```

```{r}
head(dat)
```

## Linear model

- Let's assume that there is a linear relationship between body mass and bill / flipper dimensions

$Y_{i} = a + \sum_{i}^{u} b_{i} \times X_{i}$


```{r}
my_lm_model <- lm(body_mass_g ~ 1 + bill_length_mm + bill_depth_mm + flipper_length_mm, 
   data = dat)

summary(my_lm_model)
par(mfrow = c(1,4))
plot(my_lm_model)
par(mfrom = c(1,1))
```

## Linear model with random effects
- Random effects can fit either a random intercept or random slope
- Adjust for issues of spatial temporal corellation
- use a slight different structure 

## Why perform random effects?
- we assume that there will be variability in the relationship as a component of random effect
- look at the data 

```{r}
ggplot(dat, aes(body_mass_g, group = species )) + geom_density(aes(fill= species),alpha = 0.4) + theme_bw()

```

## Building random effect models 
```{r message = F}
my_random_intercept_model <- lmer(body_mass_g ~   bill_length_mm + (1|species), data = dat)
my_random_slope_model <- lmer(body_mass_g ~   bill_length_mm + (bill_length_mm|species), data = dat)
dat$predict_random <- predict(my_random_slope_model)

ggplot(dat, aes(group = species, color = species)) + geom_point(aes(x = bill_length_mm, y = body_mass_g)) + 
  geom_line(aes(bill_length_mm, predict_random)) + theme_bw()
```

##checking models of random effects 
-check model fits
```{r}
summary(my_random_slope_model)
summary(my_random_intercept_model)
```

```{r, echo = F}
Model_Checking_data <- dat %>% 
  mutate(fit = fitted(my_random_intercept_model),
         resid_mod= resid(my_random_intercept_model),
         theorectical_quantiles = ppoints(resid_mod),
         sample_quantiles = sort(rnorm(length(resid_mod), mean(resid_mod), sd(resid_mod))))

qqplot_test <- ggplot(Model_Checking_data) + geom_point(aes(
  sample_quantiles, theorectical_quantiles
)) + theme_bw()

resid_graph <-  ggplot(Model_Checking_data) + 
  geom_point(aes(fit, resid_mod, color = species)) + geom_abline(aes(intercept = 0, slope = 0)) + theme_bw()
plot_grid(qqplot_test, resid_graph, rel_widths = c(1, 1.5))
```
- compare models with log likelihood test
```{r}
anova(my_random_slope_model, my_random_intercept_model)

MuMIn::r.squaredGLMM(my_random_intercept_model)
### R2m is fixed effects only , R2c is mixed + random 

```


-Nakagawa, S., Johnson, P.C.D., Schielzeth, H., 2017. The coefficient of determination R2 and intra-class correlation coefficient from generalized linear mixed-effects models revisited and expanded. Journal of The Royal Society Interface 14, 20170213. https://doi.org/10.1098/rsif.2017.0213




## Random Forest models

- Random Forest is a type of model that can be used for both regression and classification 

- Forest of decision trees that split the data into subgroups

For a given tree: 

- A **random** sample of the observations is taken

- At each node, **mtry** predictor variables are **randomly** selected and the tree splits the data so that resulting subsgroups are as different from each other as possible and observations falling in the same subgroup are as similar to each other as possible

- If a new observation is ran through the tree, a prediction is made

- We can calculate an out-of-bag error (OOB) on the observations left out. 

Considering the forest: 

- We can run a new observation through all the trees in the forest

- A prediction will be made for this observation by averaging (regression) or taking the most common predicted value (classification)


![](https://www.tibco.com/sites/tibco/files/media_entity/2021-05/random-forest-diagram.svg){width=80%}

## Random Forest - Regression

Let's assume that we want to predict the **body mass** of each penguin based on species, bill length, bill depth, flipper length, sex and the island where the penguin was observed. 

```{r}
predictor_vars <- dat %>%
  select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, sex, island)

rf_body_mass <- randomForest(x = predictor_vars, 
                             y = dat$body_mass_g)

rf_body_mass
```

What do `Mean of squared residuals` and `% Var explained` mean? 

- Residuals (error): $y_{i} - \hat{y}_{i}$

- Mean square error: $MSE = \frac{\sum{(y_{i} - \hat{y}_{i})^{2}}}{n}$

- Root mean square error: $RMSE = \sqrt{\frac{\sum{(y_{i} - \hat{y}_{i})^{2}}}{n}}$

- Coefficient of determination (i.e. variance explained): $R^{2} = 1 - \frac{\sum{(y_{i} - \hat{y}_{i})^{2}}}{\sum{(y_{i} - \overline{y})^{2}}}$

In the random forest output, these accuracy metrics are calculated from the **out-of-bag data**

## Random Forest - Regression

```{r}
# Calculate our own summary metrics
obs_pred_df <- data.frame(obs = rf_body_mass$y, 
                          pred = rf_body_mass$predicted)

obs_pred_df <- obs_pred_df %>%
  mutate(residual = obs - pred)

head(obs_pred_df)

# The caret package has functions to calculate R2 and RMSE
obs_pred_df %>%
  summarize(R2 = caret::R2(pred = pred, obs = obs), 
            RMSE = caret::RMSE(pred = pred, obs = obs), 
            ME = mean(residual), 
            MAE = mean(abs(residual)))

# Scatterplot of observed VS predicted values
ggplot(obs_pred_df, aes(x = obs, y = pred)) + 
  geom_point() + 
  geom_abline(linetype = "dashed", color = "red") + 
  coord_equal() + 
  theme_bw() + 
  labs(x = "Observed body mass (g)", 
       y = "Predicted body mass (g)")
```

## Random Forest - Classification

- Let's assume that we want to predict the **species** 

- You need to make sure that the response variable is a **factor** (categorical data)

```{r}
predictor_vars <- dat %>%
  select(island, bill_length_mm, bill_depth_mm, flipper_length_mm, sex, body_mass_g)

rf_species <- randomForest(x = predictor_vars, 
                             y = dat$species)

rf_species

OA = sum(diag(rf_species$confusion)) / nrow(dat)

(1 - OA)*100

```

## Random Forest - Variable importance

- It is possible to derive a measure of variable importance from a random forest model

- If all the observations for a given predictor variable are permuted, what is the impact on the accuracy of the model? 

```{r}
importance(rf_body_mass)

varImpPlot(rf_body_mass)
```

```{r}
importance(rf_species)

varImpPlot(rf_species)
```

## Training and validation sets

- It is important to assess the accuracy of a model of unseen data 

- Most obvious way is to split the observations into training and validation (testing) sets: `caret::createDataPartition()`

![](https://community.alteryx.com/t5/image/serverpage/image-id/71542i222AF143484A2306/image-size/large?v=v2&px=999){width=80%}

- We can also use *cross-validation* (e.g. k-fold CV with k = 4)

![](https://community.alteryx.com/t5/image/serverpage/image-id/71553i43D85DE352069CB9/image-size/large?v=v2&px=999){width=80%}

If interested, have a look at the `caret` or `tidymodels` package

## Save a model 

- Once you have developed a model, you can save it as an RDS file (R object)

```{r, eval=FALSE}
readr::save_rds(my_model, file = "path/to/directory/my_model.rds")
```

- You can restore it later on: 

```{r, eval=FALSE}
my_model <- readr::read_rds("path/to/directory/my_model.rds")
```

## Make predictions for new data

- We have another data frame with new observations of the predictor variables

- What is the predicted body mass? 

```{r}
new_obs <- readr::read_rds("../data/new_obs_penguins.rds")

head(new_obs)
```

```{r}
predict(rf_body_mass, new_obs)
```

## Mapped predictions - Stem density example

- We can also have gridded predictor variables

```{r}
raster_pred_vars <- terra::rast("../data/r_pred_vars.tif")

raster_pred_vars

terra::plot(terra::subset(raster_pred_vars, c("p95", "cov_2m")))
```

- We load a random forest model predicting stem density for the lidar metrics

```{r}
rf_dens <- readr::read_rds("../data/rf_dens.rds")

rf_dens
```

- The function `terra::predict` can be used to apply the model across the gridded predictor variables

```{r}
map_dens <- terra::predict(raster_pred_vars, 
                           rf_dens)

terra::plot(map_dens)
```


